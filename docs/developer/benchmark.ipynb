{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3756777c",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "ESS is built for various types of experiments with multiple instruments using different techniques.\n",
    "\n",
    "Benchmarking was needed to make sure the data reduction frameworks can handle the streaming data in real time.\n",
    "\n",
    "We will monitor the two types of computing costs, ``time`` and ``space(memory)`` of workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4d6e5",
   "metadata": {},
   "source": [
    "## Running Benchmarks\n",
    "\n",
    "Benchmark tests and related tools are collected in ``tests/benchmarks``\n",
    "and ``tests/prototypes`` of the [repository](https://github.com/scipp/beamlime).\n",
    "\n",
    "These tools and prototypes are implemented with dependency injection modules of ``beamlime``.\n",
    "\n",
    "### Benchmark Session\n",
    "\n",
    "``BenchmarkSession`` is the key helper for benchmark tests.\n",
    "\n",
    "It is a bridge object that glues \n",
    "``report: BenchmarkReport``, ``runner: BenchmarkRunner`` and ``file_manager: BenchmarkFileManager``.\n",
    "\n",
    "``runner`` is expected to be customized often for various ways of tests.\n",
    "``report`` and ``file_manager`` are not expected to be replaced as often.\n",
    "\n",
    "You can build a benchmark session object from the ``Factory``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../../'))  # To use ``tests`` as a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import Pretty\n",
    "from tests.benchmarks.runner import create_benchmark_runner_factory, BenchmarkSession\n",
    "\n",
    "benchmark_factory = create_benchmark_runner_factory()\n",
    "session = benchmark_factory[BenchmarkSession]\n",
    "Pretty(session, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75c9c3",
   "metadata": {},
   "source": [
    "Here is the simple use case of ``BenchmarkSession.run``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9eaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(x: float) -> float:\n",
    "    from time import sleep\n",
    "    sleep(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "session.run(test_func, 0.1)\n",
    "Pretty(session.report.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63a66a",
   "metadata": {},
   "source": [
    "The ``BenchmarkSession.run`` passes all arguments to ``runner`` and append its result into the ``report``.\n",
    "\n",
    "You can use ``BenchmarkSession.configure`` to temporarily update configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e056929",
   "metadata": {},
   "outputs": [],
   "source": [
    "with session.configure(iterations=2):  # Run the benchmark twice.\n",
    "    session.run(test_func, 0.2)\n",
    "\n",
    "Pretty(session.report.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d9fb3",
   "metadata": {},
   "source": [
    "Note that each iteration appends the result separately, instead of deriving average results and save the number of iteration together.\n",
    "\n",
    "It is because these tools are intended for time-consuming tests, in the scale of minutes and hours.\n",
    "\n",
    "If you need multi-thousands iterations similar to ``timeit``, you can write a special ``runner`` to do so.\n",
    "\n",
    "See [Exercies: TimeIt Runner](Exercise:-TimeIt-Runner) for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffae990",
   "metadata": {},
   "source": [
    "### BenchmarkRunner\n",
    "\n",
    "The ``BenchmarkRunner`` should be a callable that returns a ``SingleRunReport``,\n",
    "that can be appended to the ``BenchmarkReport``.\n",
    "\n",
    "Here is the simple use case of the runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26671cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.runner import BenchmarkRunner\n",
    "runner = benchmark_factory[BenchmarkRunner]  # SimpleRunner\n",
    "\n",
    "def test_func(x: float) -> float:\n",
    "    from time import sleep\n",
    "    sleep(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "single_report = runner(test_func, 0.1)\n",
    "Pretty(single_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0313f",
   "metadata": {},
   "source": [
    "``BenchmarkRunner`` is meant to be customized for various purposes and more complicated benchmarks.\n",
    "\n",
    "See ``tests/prototypes/prototype_mini.py`` and ``tests/prototypes/prototype_test.py`` for more complicated use-cases.\n",
    "\n",
    "Here is a simple exercise of customizing runners.\n",
    "\n",
    "#### Exercise: TimeIt Runner\n",
    "\n",
    "If you want to benchmark more than hundreds of iterations on the same target,\n",
    "it might not be ideal to append each result to the report.\n",
    "\n",
    "Let's write a runner that works with ``timeit``.\n",
    "\n",
    "It should also have ``iterations`` in the report.\n",
    "\n",
    "Since it is not part of arguments of the target function, it would better be added in the ``measurements``.\n",
    "\n",
    "Note that all measurement types need to have ``value`` and ``unit`` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.runner import BenchmarkRunner, SingleRunReport, TimeMeasurement, BenchmarkResult, BenchmarkTargetName\n",
    "from typing import Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Iterations:\n",
    "    value: int\n",
    "    unit: str = 'counts'\n",
    "\n",
    "\n",
    "# Extended benchmark result container.\n",
    "@dataclass\n",
    "class TimeItBenchmarkResult(BenchmarkResult):\n",
    "    iterations: Optional[Iterations] = None\n",
    "\n",
    "\n",
    "# Customized benchmark runner.\n",
    "class TimeItRunner(BenchmarkRunner):\n",
    "    def __call__(self, func: Callable, iterations: int, **kwargs) -> SingleRunReport:\n",
    "        from functools import partial\n",
    "        from timeit import timeit\n",
    "\n",
    "        target = partial(func, **kwargs)\n",
    "        result = timeit(target, number=iterations)\n",
    "\n",
    "        return SingleRunReport(\n",
    "            callable_name=BenchmarkTargetName(func.__name__),\n",
    "            arguments=kwargs,\n",
    "            benchmark_result=TimeItBenchmarkResult(\n",
    "                TimeMeasurement(result, 's'),\n",
    "                iterations = Iterations(iterations),\n",
    "            ),\n",
    "            output=target(),\n",
    "        )\n",
    "\n",
    "\n",
    "# Build the benchmark session with the customized runner and run the tests.\n",
    "with benchmark_factory.temporary_provider(BenchmarkRunner, TimeItRunner):\n",
    "    timeit_session = benchmark_factory[BenchmarkSession]\n",
    "\n",
    "\n",
    "timeit_session.run(test_func, 100, x=0.001)\n",
    "Pretty(timeit_session.report.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff7105",
   "metadata": {},
   "source": [
    "### Benchmark Report\n",
    "\n",
    "The ``report`` is a container of benchmark results.\n",
    "\n",
    "``BenchmarkReport`` should have ``append`` method that receives a ``SingleRunReport`` and save in itself.\n",
    "\n",
    "It is not expected to be customized so often so it has its own implementation.\n",
    "\n",
    "It has four dataclass fields, ``environment``, ``target_names``, ``measurements`` and ``arguments``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90908544",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pretty(session.report, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d571993",
   "metadata": {},
   "source": [
    "``environment`` is a static field that has information of the hardware it is running on,\n",
    "and other fields contain benchmark results.\n",
    "\n",
    "``target_names``, ``measurements`` and ``arguments`` are similar to ``Series`` or ``DataFrame`` of ``pandas``.\n",
    "\n",
    "It is for exporting the report as a ``pandas.DataFrame``.\n",
    "The exported ``pandas.DataFrame`` will be then converted to\n",
    "``scipp.Dataset`` for further visualization with ``plopp``.\n",
    "\n",
    "Here is the example of the full contents of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82234cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the benchmark report.\n",
    "Pretty(session.report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e05d2",
   "metadata": {},
   "source": [
    "### Benchmark File Manager\n",
    "\n",
    "``BenchmarkFileManager.save`` should receive a ``BenchmarkReport`` and save it.\n",
    "\n",
    "It is also not expected to be customized very often.\n",
    "\n",
    "By default, it saves a result under ``.benchmarks/`` directory as a json file.\n",
    "\n",
    "If you want to choose a different directory to save the benchmark result, replace the provider of ``BenchmarkRootDir``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c85aa",
   "metadata": {},
   "source": [
    "## Benchmark Session as a Pytest fixture.\n",
    "\n",
    "You can use a benchmark session as a pytest fixture.\n",
    "See ``conftest.py`` under ``tests`` for available pytest flags.\n",
    "\n",
    "The following example is how to write a fixture.\n",
    "\n",
    "If the fixture has a scope of ``function``, each result will be saved in a new file in this example.\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from tests.benchmarks.runner import BenchmarkSession, SimpleRunner, create_benchmark_runner_factory, BenchmarkRunner\n",
    "from typing import Generator, Any\n",
    "factory = create_benchmark_runner_factory()\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def benchmark_session() -> Generator[BenchmarkSession, Any, Any]:\n",
    "    with factory.temporary_provider(BenchmarkRunner, SimpleRunner):\n",
    "        session = factory[BenchmarkSession]\n",
    "        yield session\n",
    "        # Save when the pytest session is over.\n",
    "        session.save()\n",
    "\n",
    "def a_function_you_want_to_test() -> None:\n",
    "    ...\n",
    "\n",
    "\n",
    "def test_prototype_benchmark(benchmark_session: BenchmarkSession) -> None:\n",
    "    with benchmark_session.configure(iterations=100):  # Run the test 100 times.\n",
    "        benchmark_session.run(a_function_you_want_to_test)\n",
    "\n",
    "    # Save when a single test is over.\n",
    "    benchmark_session.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce7ca92",
   "metadata": {},
   "source": [
    "## Customizing Benchmark Providers.\n",
    "\n",
    "You can customize the benchmark tools by replacing providers.\n",
    "\n",
    "For example, you can customize a benchmark report by adding more fields or removing unnecessary fields.\n",
    "\n",
    "Let's use a smaller subset, ``HardwareSpec`` for exercises.\n",
    "\n",
    "### Exercise 1: Add an extra field into ``HardwareSpec``.\n",
    "To have an extra field, the customized type of ``HardwareSpec`` should be\n",
    "\n",
    "1. A subclass of the original type ``HardwareSpec``\n",
    "2. Decorated as a ``dataclass``\n",
    "\n",
    "(1) is to keep the child class compatible with ``HardwareSpec`` as a provider of ``HardwareSpec``,\n",
    "(It only allows if it is a subclass or itself.) and\n",
    "(2) is to keep the child class compatible with ``asdict`` of ``dataclass`` in the ``BenchmarkReport``.\n",
    "\n",
    "See the following example of implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.environments import HardwareSpec, env_providers\n",
    "from beamlime.constructors import Factory\n",
    "from dataclasses import dataclass\n",
    "from typing import NewType\n",
    "from copy import copy\n",
    "\n",
    "minimum_env_providers = copy(env_providers)\n",
    "\n",
    "TMI = NewType(\"TMI\", str)\n",
    "\n",
    "# This class can't be decorated as a provider since it is a provider of its parent type.\n",
    "@dataclass\n",
    "class MyHardwareSpec(HardwareSpec):\n",
    "    extra_info: TMI = TMI(\"A little more information.\")\n",
    "\n",
    "# ``MyHardwareSpec`` should be explicitly registered as a provider.\n",
    "minimum_env_providers.pop(HardwareSpec)\n",
    "minimum_env_providers[HardwareSpec] = MyHardwareSpec\n",
    "custom_env_factory = Factory(minimum_env_providers)\n",
    "\n",
    "Pretty(custom_env_factory[HardwareSpec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5a59b",
   "metadata": {},
   "source": [
    "### Exercise 2: Remove ``CPUSpec`` from ``HardwareSpec``.\n",
    "If you want to ``remove``/``exclude`` a field, there are more steps needed than just overwriting an existing ones.\n",
    "\n",
    "Here are the options.\n",
    "\n",
    "1. Annotate the field as ``Optional`` and remove the provider.\n",
    "   Please note that the field will be populated if there is a provider in the provider group.\n",
    "   The field will be set as ``None`` so it is not completely removed.\n",
    "2. Replace the provider with another class without the field.\n",
    "   It should not inherit the original class.\n",
    "   Note that the users of this class also need to be updated in this case.\n",
    "\n",
    "See the following examples of removing ``CPUSpec`` from ``HardwareSpec``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.benchmarks.environments import CPUSpec, BenchmarkEnvironment\n",
    "from typing import Optional\n",
    "\n",
    "# 1. Annotate the field as ``Optional`` and remove the provider.\n",
    "\n",
    "optional_env_providers = copy(env_providers)\n",
    "\n",
    "optional_env_providers.pop(CPUSpec)\n",
    "\n",
    "@dataclass\n",
    "class LessHardwareSpec(HardwareSpec):\n",
    "    cpu_spec: Optional[CPUSpec] = None\n",
    "\n",
    "\n",
    "optional_env_providers.pop(HardwareSpec)\n",
    "optional_env_providers[HardwareSpec] = LessHardwareSpec\n",
    "\n",
    "Pretty(Factory(optional_env_providers)[BenchmarkEnvironment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import make_dataclass, fields\n",
    "\n",
    "# 2. Replace the provider with another class without the field.\n",
    "replaced_env_providers = copy(env_providers)\n",
    "\n",
    "RHS = make_dataclass(\n",
    "    \"ReplacedHardwareSpec\",\n",
    "    [(field.name, field.type, field) for field in fields(HardwareSpec) if field.type != CPUSpec]\n",
    ")\n",
    "\n",
    "@replaced_env_providers.provider\n",
    "class ReplacedHardwareSpec(RHS):\n",
    "    ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReplacedBenchmarkEnvironment(BenchmarkEnvironment):\n",
    "    hardware_spec: ReplacedHardwareSpec  # Hardware spec is overwritten.\n",
    "\n",
    "\n",
    "replaced_env_providers.pop(BenchmarkEnvironment)\n",
    "replaced_env_providers[BenchmarkEnvironment] = ReplacedBenchmarkEnvironment\n",
    "\n",
    "Pretty(Factory(replaced_env_providers)[BenchmarkEnvironment])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda4254",
   "metadata": {},
   "source": [
    "### Exercise 3: Empty Ancestor for Easy Customization.\n",
    "\n",
    "If you often need to customized providers, consider updating the original one.\n",
    "If you don't want to include/remove fields from the original one but still have to customize them often,\n",
    "consider adding an extra ancestor class that contains nothing and annotate the frequently customized fields of its users.\n",
    "\n",
    "Not all types are implemented this way from the beginning to avoid too many layers of inheritance and complicated code base.\n",
    "\n",
    "For example, ``PrototypeRunner`` is expected to be replaced often.\n",
    "``PrototypeRunner`` works as a ``Protocol`` or an ``Interface`` and the ``BenchmarkSession.runner`` is annotated with this type.\n",
    "So users need to implement a subclass and explicitly set it as a provider of the ``PrototypeRunner``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beamlime.constructors import ProviderGroup\n",
    "import os\n",
    "\n",
    "# Update the original types as following.\n",
    "env_providers = ProviderGroup()\n",
    "\n",
    "OsInfo = NewType(\"OsInfo\", str)\n",
    "env_providers[OsInfo] = lambda: OsInfo(os.uname().version)\n",
    "\n",
    "class HardwareSpec:\n",
    "    ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultHardwareSpec(HardwareSpec):\n",
    "    os_info: OsInfo\n",
    "\n",
    "\n",
    "# ``HardwareSpec`` needs to be explicitly registered as a provider of ``HardwareSpecAncestor``.\n",
    "env_providers[HardwareSpec] = DefaultHardwareSpec\n",
    "\n",
    "@env_providers.provider\n",
    "@dataclass\n",
    "class BenchmarkEnvironment:\n",
    "    hardware_spec: HardwareSpec\n",
    "\n",
    "\n",
    "Pretty(Factory(env_providers)[BenchmarkEnvironment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the provider of ``HardwareSpecAncestor``.\n",
    "updated_providers = copy(env_providers)\n",
    "\n",
    "@dataclass\n",
    "class EasilyUpdatedHardwareSpec(HardwareSpec):\n",
    "    extra_info: TMI = TMI(\"A little more information.\")\n",
    "\n",
    "# Then it is much easier to replace the provider,\n",
    "# Since the user class doesn't need to be updated.\n",
    "updated_providers.pop(HardwareSpec)\n",
    "updated_providers[HardwareSpec] = EasilyUpdatedHardwareSpec\n",
    "\n",
    "Pretty(Factory(updated_providers)[BenchmarkEnvironment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
